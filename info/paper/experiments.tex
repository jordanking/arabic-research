\section{Experiments}

We performed a broad parameter sweep over various preprocessing techniques and word2vec parameterizations to determine the optimal word embedding methods. We evaluated each parameterization on two tasks - one for semantic similarity, and one for syntactic understanding. The text corpus for training the embeddings is a cleaned Arabic Wikipedia dump. Each part of this experiment is described in the following subsections.

\subsection{Semantic Understanding Evaluation}

The semantic similarity task consists of 1000 Arabic word pairs and a similarity score in the range 0-10, where 10 represents words that are extremely related. As no task existed of the size that we required for our parameter sweep, we developed this task ourselves. 
\\
The words began by selecting 1250 of the most common words from the Arabic Wikipedia, excluding words that occur in more than $5\%$ of sentences. These words word then translated into English with Google translate, queried against the big huge thesaurus API for either synonyms or antonyms, and translated back to Arabic. The original word and the resulting word are then paired up. One half of the pairs are synonyms, one quarter are antonyms, and one quarter are shuffled with other pairs to be randomly matched. The proportions were chosen because the synonyms database is more extensive than the antonym database. The various APIs involved introduce a large amount of noise, to the point that some synonym pairs will be completely unrelated Arabic words. We take advantage of this noise to distribute the relatedness of words across the 0-10 scale, while ensuring some pairs are related.
\\
This list of word pairs was then given to 10 fluent Arabic speakers, along with simple instructions to evaluate the relatedness of the words on a scale of 1 to 5. The values that they provided were then averaged and scaled to 0 to 10. When evaluating a parameterization, we performed the same preprocessing on the word list as we did to the corpus prior to training. Each word pair's embeddings were compared for a similarity score, and the parameterization recieved a score for its squared error off the task's similarity score.

\subsection{Syntactic Understanding Evaluation}

The syntactic understanding of the word embeddings was evaluated via a part-of-speech tagging task. A selection of Arabic documents were first tagged with part-of-speech values using Madamira's NLP analysis, once for each preprocessing method. For each parameterization, a simple recurrent neural network is trained to predict the part-of-speech of a word using its embedding. One document was held out as a test set for the network, and the accuracy of the network on this set was taken as the syntactic understanding score for the parameterization.

\subsection{Preprocessing Options}

The three main preprocessing options that we consider for this task are as-is, tokenized, and lemmatized. As-is leaves the corpus as it is. Tokenization breaks each word into simple grammatical tokens, separating the definite article and pronoun suffixes from the word. Lemmatization completely removes such affixes from the corpus, mapping each word to a base word that represents the simple meaning of the word. It reduces words to a single tense, gender, and definiteness while preserving grammatical forms.

Further considered preprocessing parameters are normalizing Arabic text, removing diacritics and reducing multiple forms of equivalent letters to a single letter.

\subsection{Parameterizations}

The main parameters of word2vec that are considered are window size, dimension, and algorithm. The window sizes considered are 5 and 8. The dimensions considered are 100 and 200. Both CBOW and Skipgram algorithms are considered.

\subsection{Implementation}

All operations defined above utilize the Arapy package we developed, which is released as an open source utility. All preprocessing options were precomputed first, generating multiple copies of the Arabic Wikipedia corpus. Then word vectors were generated for each parameterization. The vectors were then ran through both evaluation tasks, recording their performance.

\subsection{Results}

The final results are shown here. Notably, parameter xxx was the best performer on both tasks, suggesting that the method of preprocessing was much better.

\label{sec:experiments}
