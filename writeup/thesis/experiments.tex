
\section{Experimental Results}
\label{sec:experiments}

We perform a parameter sweep over the various preprocessing techniques, normalization options, and word2vec parameterizations to determine the optimal word embedding methods. The text corpus for training the embeddings is an Arabic Wikipedia dump from \url{https://dumps.wikimedia.org/arwiki/20150901/} \cite{wiki:xxx}, cleaned by dropping Wikipedia markup, punctuation, and non-Arabic characters. All preprocessing options are precomputed first, generating multiple versions of the Arabic Wikipedia corpus. Then word vectors are trained for each parameterization. The vectors are then ran through evaluation tasks, recording various performance statistics.
\\
The baseline results for English vectors are shown in Table \ref{table:englishtask}. There are two models shown, each evaluated on the WS353 English word similarity task. The first is an English model trained under a default parameterization (skipgram, window of 7, 100 dimensions) on the same number of words as our Arabic models. The second is the publically available pre-trained vectors trained on the 300 billion word Google News Corpus. The metrics that we choose to display are mean error and \textcolor{red}{correlation} with the evaluation task. The mean error is the mean absolute distance between the vector estimate and the evaluation task estimate for word pair similarity. The default vectors exhibit an impressive .268 mean error, and both models show a high correlation with the evaluation task scores. The Google News vectors display an impressive $.65$ correlation score to the task, showing the power of well-trained word vectors.

\begin{table}
\begin{tabular}{l|l|l}
\bfseries Method & \bfseries Mean Error & \bfseries Correlation
\csvreader[head to column names]{results/1-en.csv}{}
{\\\hline\csvcoli&\csvcoliii&\csvcolv}
\end{tabular}
\caption{Baseline English Results}
\label{table:englishtask}
\end{table}

Tables \ref{table:ourtask} and \ref{table:ws353task} show the results of models trained on differently preprocessed text. Table \ref{table:ourtask} contains results from the similarity task that we developed, and Table \ref{table:ws353task} contains the results from the WS353 Arabic task \cite{finkelstein:2001,hassan:2009}. None of the Arabic models reach either the level of correlation or accuracy that the default English model does. These tabels are sorted by mean error, revealing traits that improved their performance on evaluation tasks. We analyze these results in Section \ref{section:recommendations}.



% \begin{table}
% \begin{tabular}{l|l|l}%
% \bfseries Method & \bfseries Mean Error & \bfseries Correlation
% \csvreader[head to column names]{results/ar_similiarity_task_results_sorted_Accuracy.csv}{}
% {\\\hline\csvcoli&\csvcoliii&\csvcolv}
% \end{tabular}
% \caption{Results on Our Task}
% \label{table:ourtask}
% \end{table}

% \begin{table}
% \begin{tabular}{l|l|l}%
% \bfseries Method & \bfseries Mean Error & \bfseries Correlation
% \csvreader[head to column names]{results/ar_similiarity_task_results_ws353_sorted_Accuracy.csv}{}
% {\\\hline\csvcoli&\csvcoliii&\csvcolv}
% \end{tabular}
% \caption{Results on ws353 Task}
% \label{table:ws353task}
% \end{table}

