\section{Training Word Embeddings in Arabic}

There are a number of decisions to be made when training word embeddings in Arabic. We have chosen to use the word2vec framework to train, although there are other proposed methods to obtain word embeddings. \textcolor{red}{Elaborate other training methods a bit.} The main decisions to be made when training word embeddings in Arabic are how to preprocess the text, how to normalize the text, and how to parameterize the word2vec algorithms.

\subsection{Preprocessing Options}

Preprocessing is very important when analyzing Arabic text. Much of the linguistic information in the grammar is contained in various affixes to words. This is very different from English, where information is often contained in stand-alone pronouns and articles. Word2vec captures information at a word level, so separating these affixes into individual words greatly changes what is learned during training.
\\
The three main preprocessing options that we consider for this task are 1) leave the text unedited, 2) tokenize the text to make affixes individual words, and 3) lemmatize the text to drop most affixes and preserve only the core idea of each Arabic word.Tokenization breaks each word into simple grammatical tokens and creates separate words from affixes such as the definite article and the various pronouns. Lemmatization completely removes such affixes from the corpus, mapping each word to a base word that represents the core meaning of the word. It reduces words to a single tense, gender, and definiteness, but preserves the basic grammatical form. An English equivalent would be to map both \textit{he jumped} and \textit{she jumps} to \textit{he jumps}.

\subsection{Normalization}

Normalizing Arabic text can greatly reduce the sparsity of the word space in Arabic. We always normalize the corpus by removing English characters, reducing all forms of the letters alif, hamza, and yaa to single general forms (respectively \textcolor{red}{ุง,ุก,and ู -- arabic letters not showing up?}). The options we consider variable are removing diacritics and reducing both English and Arabic numerical characters to the number sign.

\subsection{Parameterizations}

The main parameters of word2vec that are considered are algorithm, embedding dimension, and window size. Both CBOW and Skipgram algorithms are considered. The embedding dimensions considered are 100 and 200. We chose these dimensions as the typical range is between 100 and 300, where 300 requires more time and data to train. We lack the gratuitous amount of data that is available freely for English, so we chose to keep only the smaller two dimensionalities. The window sizes considered are 5 and 8, which is how far to either side of the word being trained we look for context. For the other parameters, refer to Table \ref{table:params}.

\begin{table}
\begin{tabular}{l|l|l}
\textbf{Parameter} & \textbf{Value} & \textbf{Explanation} \\
\hline
$sg$ & $[0,1]$ & Algorithm \\
$size$ & $[100, 200]$ & Dimensionality \\
$window$ & $[4, 7]$ & Context window \\
$min count$ & $5$ & Filters rare words \\
$sample$ & $1e-5$ & Downsampling \\
$seed$ & $1$ & Random seed \\
$hs$ & $1$ & Heirarchical samp. \\
$negative$ & $0$ & Negative samp. \\
$iterations$ & $5$ & Training iterations \\
\end{tabular}
\caption{Training Parameters}
\label{table:params}
\end{table}