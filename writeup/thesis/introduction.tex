\section{Introduction}
\label{sec:introduction}


Arabic word embeddings are numerical vector representations of a word's meaning - both semantic meaning and syntactic meaning. These embeddings are obtained using machine learning algorithms - word2vec - that utilize the context a word appears in to infer its meaning \cite{mikoloveffic:2013,mikolovdist:2013}. This works very well as words with similar meanings tend to be used in similar contexts, which are defined by the preceeding and following $n$ words. For example, the sentences \textit{I eat bread every night} and \textit{I eat rice every night} are examples of how food words may appear in similar contexts. With enough text to process, we can train numerical vectors to learn that bread and rice appear in these \textit{common-for-food} contexts. Similarly, we can learn syntactic relationships because different parts of speech appear in certain context patterns as well.
\\
High quality word embeddings provide a representation of the meaning of a word, without ever translating or referencing a dictionary. We can obtain the semantic and syntactic meaning directly from a corpus of natural written language. With accurate word embeddings, we can perform powerful vector operations to investigate the relationships between words in a corpus. A few of the possible operations are measuring the similarity of two words, identifying which word from a set is least similar, and solving basic analogies \cite{mikolovdist:2013}. The classic demonstration of word embeddings is to take (the embeddings of) \textit{king}, subtract \textit{man}, and add \textit{woman}. The resulting embedding should be near to the embedding for \textit{queen} in the embedding vector space. Intuitively, this allows us to subtract the male gender meaning from king's embedding, add the female gender meaning, and end up with an embedding equivalent to queen's embedding \cite{mikolov2013linguistic}.
\\
While word embeddings have been analyzed and evaluated for different tasks in English corpa \cite{mikoloveffic:2013,dos2014deep}, word embeddings in other languages have received less attention. Arabic word embeddings have been included in multi-lingual work on embeddings like generic part-of-speech tagging \cite{al:2013}. However, the process of training word vectors for a language so morphologically different from English has not been explored. Every language has different levels of morphological complexity. This complexity may have a significant effect on how the word embeddings should be trained and the tasks that the word embeddings can be used for. According to one of the only papers attempting to quantify the Arabic vocabulary, Arabic itself has around 250 prefixes, 4500 regular derivative roots, 1000 derivative regular forms, and 550 suffixes to build words from, meaning there are around $6*10^{10}$ possible Arabic words \cite{ahmed2000alarge}. While the number of sensical words is estimated to be between 12 and 500 million words by unofficial sources \cite{Lameen:2013,Muhammad:2015}, the vocabulary is much larger than the 1 million word vocabulary of English \cite{Googl96:online}. In this paper, we focus on different preprocessing and training parameters to bring the performance of Arabic word embeddings closer to that of English word embeddings.
\\
Having word embeddings in different languages allows us to avoid translation when using these embeddings for different natural language processing and machine learning tasks. These embeddings can allow for the interpretation of topics in media, or provide an understanding of how similar words are to a subject. An example application would be to use the embeddings to learn what words are highly similar to words representing fear, and then compute the degree to which some media is using fearful language in the context of political or economic termoil.
\\
Methodologies and properties of English word embeddings have been extensively researched, however little attention has been given to the production and application of Arabic word embeddings. Written Arabic words often carry more contextual information about objects, tense, gender, and definiteness than English, meaning that Arabic unigrams occur less frequently on average than English unigrams. For example, the sentence \<هو حمله الي بيتها> translates to \textit{He carried it to her house} The word \<بيتها> for example, is a combination of \<بيت> (\textit{house}) and \<ها> (\textit{her}). This has a significant effect on the training and application of Arabic word embeddings as the embeddings are trained on unigram tokens. The complex words then occur with less frequency and more semantic meaning than the English counterparts of \textit{house} and \textit{her}.
\\
The contributions of this work are as follows: 1) We perform a comparative empirical evaluation of Arabic and English word vectors using both a semantic similarity task and a syntactic similarity task. We show that standard parameters for English word embeddings lead to poor Arabic word embeddings. 2) We developed an open-source software package that provides easy access to important Arabic natural language processing tools. 3) We evaluate this task using more high quality human labels than other similar evaluations. 4) We present an empirical analysis identifying the parameters that are most effective for our tasks, identifying a set of best practices for training Arabic word embeddings.
\\
The remainder of this paper is organized as follows. In Section \ref{sec:literature} we present work related to training, evaluating, and utilizing Arabic word vectors. In Section \ref{sec:training} we provide an overview of the process and parameters required to train word vectors in Arabic. Section \ref{sec:evaluation} describes our methodology to measure the quality of word vectors against a semantic similarity task. In Section \ref{sec:experiments} we present the results of our parameter sweep and evaluations of Arabic word vectors. This leads to Section \ref{sec:recommendations} in which we inspect our results to find the best methods for creating Arabic word vectors according to our evaluation tasks. Following this we describe the software created to perform our analyses in Section \ref{sec:arapy}. Section \ref{sec:conclusion} offers our thoughts on further work and conclusions.
