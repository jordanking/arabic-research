\section{Evaluating Arabic Word Embeddings}

It is a complex problem to evaluate the quality of word embeddings. The word2vec methods produce unsupervised vectors that maximize the probability of predicting a word given the context that it appears near in the training corpus. 

%This context provides both semantic and syntactic information about each word. 

%As our preprocessing methods may have significant effects on how much of this information is provided, we use two evaluation tasks to understand the semantic and syntactic quality of our embeddings.

%\subsection{Semantic Understanding Evaluation}

As we perform a large parameter sweep in \ref{sec:experiments}, we want a large semantic similarity task to accurately evaluate the Arabic word embeddings. The largest Arabic semantic similarity task that we could find is a manually translated version of the WordSimilarity-353 task \cite{finkelstein:2001,hassan:2009}. As we wanted a larger list generated specifically for Arabic and evaluated by multiple fluent Arabic speakers, we created a list of 1000 similarity scores for given Arabic word pairs using fluent Arabic speakers. The semantic similarity task consists of 1000 Arabic word pairs given a similarity score in the range 0-1. Pairs with a score of 1 indicates that the words are extremely related.
\\
To begin creating this task, we selected 1250 of the most common words in the Arabic Wikipedia dump \textcolor{red}{find out how to cite this, fix the following citations}, excluding words that occur in more than $5\%$ of sentences. These words were then translated into English with Google translate \cite{google:online}, queried against the big huge thesaurus API for either synonyms or antonyms, and translated back to Arabic \cite{bhl:online}. The original word and the resulting synonym or antonym are then paired up. Half of the pairs are at this point synonyms, one quarter are antonyms, and one quarter are shuffled with other pairs to be randomly matched. This distribution is synonym heavy because the synonym database is more extensive and accurate than the antonym database. The various APIs involved introduce a large amount of noise, to the point that some synonym pairs will be completely unrelated Arabic words. We take advantage of this noise to attempt to distribute the relatedness of words across the 0 to 1 scale.
\\
This list of word pairs was then given in parts to fluent Arabic speakers such that each pair is scored by multiple evaluators. We provided simple instructions to evaluate the relatedness of the words on a scale of 0 to 5. The values that they provided were then scaled from 0 to 1 and averaged. When evaluating a parameterization, we performed the same preprocessing on the word list as we did to the corpus prior to training. Each word pair's embeddings were compared for an absolute cosine similarity score and the parameterization is given a score for its \textcolor{red}{mean absolute difference} from the task's similarity score.

% \subsection{Syntactic Understanding Evaluation}

% The syntactic understanding of the word embeddings was evaluated via a part-of-speech tagging task. A selection of Arabic documents were first tagged with part-of-speech values using Madamira's NLP analysis, once for each preprocessing method \cite{pasha:2014}. For each parameterization, a simple recurrent neural network \textcolor{red}{set up for sequence to sequence learning} is trained to predict the part-of-speech of a word using its embedding. \textcolor{red}{One document} is held out as a test set for the network, and the accuracy of the network on this set was taken as the syntactic understanding score for the parameterization.