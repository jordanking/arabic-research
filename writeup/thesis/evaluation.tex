\section{Evaluating Arabic Word Embeddings}
\label{sec:evaluation}


It is a complex problem to evaluate the quality of word embeddings. The word2vec methods produce unsupervised vectors that maximize the probability of predicting a word given the context that it appears near in the training corpus. 

%This context provides both semantic and syntactic information about each word. 

%As our preprocessing methods may have significant effects on how much of this information is provided, we use two evaluation tasks to understand the semantic and syntactic quality of our embeddings.

%\subsection{Semantic Understanding Evaluation}

% As we wanted a larger list generated specifically for Arabic and evaluated by multiple fluent Arabic speakers, w

We want a large semantic similarity task to accurately evaluate the Arabic word embeddings. The largest Arabic semantic similarity task that we could find is a manually translated version of the WordSimilarity-353 task \cite{finkelstein:2001,hassan:2009}. We created a semantic similarity task consisting of 1000 word pairs with similarity scores. Between 2 and 5 fluent Arabic speakers labeled each word pair with a similarity score in the range 0-1, where pairs with a score of 1 indicates that the words are extremely related.
\\
To begin creating this task, we selected 1250 of the most common words in the Arabic Wikipedia dump \cite{wiki:xxx} at \url{https://dumps.wikimedia.org/arwiki/20150901/}, excluding words that occur in more than $5\%$ of the sentences. The remaining words were then translated into English with Google translate, queried against the Big Huge Thesaurus API for either synonyms or antonyms, and translated back to Arabic \cite{google:online,bhl:online}. The original word and the resulting synonym or antonym were then paired up. Half of the pairs are at this point synonyms, one quarter are antonyms, and one quarter are shuffled with other pairs to be randomly matched. This distribution is synonym heavy because the Big Huge Thesaurus database has more data on synonyms than antonyms. The various APIs involved introduce a large amount of noise, to the point that some synonym pairs end up as unrelated Arabic words. We take advantage of this noise to distribute the relatedness of words across the 0 to 1 scale.
\\
This list of 1250 word pairs was then distributed to fluent Arabic speakers such that each pair is scored by multiple evaluators. We provided simple instructions to evaluate the relatedness of the words on a scale of 0 to 5 for ease of labeling. The values that they provided were then scaled from 0 to 1 and averaged. 
\\
When evaluating a model parameterization with the WordSimilarity-353 task or our similarity task, we perform the same preprocessing on the word pairs as we do on the training corpus for each model. Each word pair's embeddings are first obtained from the model, and then an absolute cosine similarity score is obtained between them. The cosine similarity is compared against the similarity task's score. The model is scored on both the mean absolute difference between the scores and the correlation between the task scores and model scores.


% 0.7022335424


% \subsection{Syntactic Understanding Evaluation}

% The syntactic understanding of the word embeddings was evaluated via a part-of-speech tagging task. A selection of Arabic documents were first tagged with part-of-speech values using Madamira's NLP analysis, once for each preprocessing method \cite{pasha:2014}. For each parameterization, a simple recurrent neural network \textcolor{red}{set up for sequence to sequence learning} is trained to predict the part-of-speech of a word using its embedding. \textcolor{red}{One document} is held out as a test set for the network, and the accuracy of the network on this set was taken as the syntactic understanding score for the parameterization.