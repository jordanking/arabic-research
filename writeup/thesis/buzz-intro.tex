\chapter{Applying Word Embeddings to Buzz Detection}
\label{sec:buzz}

\section{Buzz Detection Methodology}

The properties of high quality word embeddings can used in unique text mining applications. In this work we apply the best models from our training experiments to a buzz detection task, where we harness the power of Arabic word embeddings to demonstrate their power. We define buzz to be the level of discussion about a semantic topic in a set of documents. We define the buzz detection task as given a set of documents and a topic, accurately identify the level of buzz about the topic within them. Our goal in the buzz task is to detect buzz over a series of months in a set of Arabic news documents relevant to Iraq. We will detect buzz for each month over which we have data, measuring how much buzz about the topic of \textit{violence} is in the news each month. Our ground truth for evaluation is the publicly available Iraq Death Count data from \url{https://www.iraqbodycount.org/database/} that aims to document how many violent deaths occur in Iraq on a monthly basis \cite{IraqB68:online}. We will look at five methods to measure buzz to demonstrate the benefits of using Arabic word embeddings, each outlined in Table \ref{table:buzztypes}. We score the results from each method against the ground truth by finding the Spearman correlation between the method's scores and the ground truth counts. We propose that we can obtain improved results on the buzz detection task through the use of Arabic word embeddings. Below we outline three baseline buzz detection methods that do not use embeddings and two buzz detection methods that utilize Arabic word embeddings.

\\

Our three baselines for buzz detection are the frequency method, the domain method, and the synonym method. The frequency method is to simply count the number of times the Arabic words for violent or violence occur per month in the corpus, and normalize this score by the total number of words in the corpus per month. The domain method is the same as the frequency method, but we use a list of violence-related Arabic words instead of just violent and violence. This list of words in the violence domain is provided by experts on social science in the Middle East. The synonym method expands this domain list by adding 5 synonyms obtained from a thesaurus (through translation) for every expert provided word. When expanding words in this way, we only keep the intersection of each of these expansions leaving us with $|expanded set| \leq |original set| * 6$. The similarity method expands the domain method by using the 5 most similar word embeddings to the word embedding for each word in the expert domain list. The weighted similarity method adds weights to each word in the similarity method by how similar it is to the word embedding for the original expert-provided word. Both methods that use word embeddings are evaluated with three different models to demonstrate differences between training methods. The three models are the top performing models on the Word Similarity 353 task, our similarity task, and the analogy task. This provides us with representation for each of the three preprocessing methods.

\begin{table}
\begin{center}
\begin{tabular}{l|l|l|l}
\textbf{Type} & \textbf{Target Words} & \textbf{Uses Embeddings} & \textbf{Weighted} \\
\hline
$frequency$ & \textit{violent, violence} & No & No \\
$domain$ & \textit{expert domain} & No & No \\
$synonym$ & \textit{expert domain + synonyms} & No & No \\
$similarity$ & \textit{expert domain + similar embeddings} & Yes & No \\
$weighted$ & \textit{expert domain + similar embeddings} & Yes & Yes \\
\end{tabular}
\caption{Methods to Measure Buzz}
\label{table:buzztypes}
\end{center}
\end{table}

\section{Buzz Detection Experiments}
\label{sec:experiments-buzz}

Table \ref{table:buzzresults} shows the correlations between each of our buzz methods and the ground truth values. \textcolor{red}{The results shown below are bad and led us to discover that the corpus did not overlap the ground truth correctly. Soon to be replaced with appropriate data from new experiments on a better corpus.}
\\
Here we can see that any method utilizing the expert domain list outperforms the base frequency method by the same amount. This is only because the corpus had only a few occurances of the target words in it. 

\begin{table}
\begin{center}
\begin{tabular}{l|l|l|l}
\bfseries Method & \bfseries Model & \bfseries Buzz & \bfseries Normalized Buzz
\csvreader[head to column names]{results_buzz/masterPrepared.csv}{}
{\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv}
\end{tabular}
\caption{Correlation Scores Between Method Scores and Ground Truth}
\label{table:buzzresults}
\end{center}
\end{table}












