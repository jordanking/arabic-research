\chapter{Recommendations for Arabic Word Embeddings}
\label{sec:recommendations}

Our results have affirmed that there is no free lunch when choosing methods to train Arabic word embeddings. There is no parameterization that outperformed all others on our tasks, but we have shown that within a task different models can offer dramatically different performance. When developing Arabic word embeddings that will be used in a context requiring performance at a level comparable to English word embeddings, it is necessary to inspect all preprocessing methods available. However, we do offer some guidelines based on our results.
\\
For preprocessing, we believe that unprocessed Arabic may perform well if it is trained on the same data on which it is applied. Due to the emphasis on syntactic analogies in the analogy task, we suggest trying lemmatization for tasks requiring syntactic analysis. We suspect that it performs well as it reduces complex words to simpler forms the retain their basic syntactic structure. For semantic-heavy analysis, we suggest trying tokenization as it performed so well on the Word Similarity 353 similarity task. Tokenization likely performs well as it does not reduce the text, but isolates each core word in a broken down context. However, we reiterate that we believe it is essential to try at least one model from each of these methods on a specific application, as they have been shown to perform very differently across different tasks.
\\
For normalization, we saw nearly no difference when we removed vowels or normalized numerical digits. As much written Arabic does not have vowels and leaving digits is situationally useful, we suggest not normalizing either of these. For training, we did not find a dominant training algorithm between Skip-Gram and CBOW. However, we do believe the smaller window size of 4 demonstrated significantly better results globally. We also found large improvements on the analogy task for 200 dimensional embeddings, and no evidence of draw backs on other tasks. With more data and time, it may be possible to obtain even better performance with 300 dimensions, as the Google News embeddings showed on the analogy task. Other parameters did not show significant improvements on any of the evaluation tasks.