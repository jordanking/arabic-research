\section{Recommendations for Arabic Word Embeddings}
\label{sec:recommendations}

The least accurate results for both Arabic semantic similarity tasks were those models trained on unprocessed Arabic text. Additionally, the accuracies of unprocessed text were far below the English default baseline. This goes to show that Arabic word embeddings do benefit in semantic accuracy when preprocessed to tokens and lemmas. Additionally, tokenized Arabic tends to have a higher accuracy on the tasks than lemmatized Arabic. \\
Also to note is the effect of normalizing digits and tashkil (voweling). When lemmatizing the Arabic, there seems to be a consistent increase in performance if you then remove tashkil. Tokenization and unprocessed Arabic do not seem to have the same improvements with lemmatization. \\


% \textcolor{red}{The best results for the syntactic similarity task are xxx.}
% \textcolor{red}{Other recommendation we have are xxx.}