
\section{Experimental Results}
\label{sec:experiments}

We performed a broad parameter sweep over the various preprocessing techniques, normalization options, and word2vec parameterizations to determine the optimal word embedding methods. The text corpus for training the embeddings is an Arabic Wikipedia dump cleaned with our Arapy package.
\\
All preprocessing, normalization, and training processes utilize the Arapy package we developed, which is released as an open source utility. It utilizes various software resources, including gensim, Madamira, the Google Translate API, the Big Huge Thesaurus API, \textcolor{red}{etc. CITE THESE}. All preprocessing options are precomputed first, generating multiple versions of the Arabic Wikipedia corpus. Then word vectors are trained for each parameterization. The vectors are then ran through evaluation tasks, recording various performance statistics.
\\
\textcolor{red}{The final results are shown here.}

\begin{table}
\begin{tabular}{l|l|l}%
\bfseries Method & \bfseries Mean Error & \bfseries Correlation
\csvreader[head to column names]{results/en_sim_demo_sorted.csv}{}
{\\\hline \csvcoli&\csvcoliii&\csvcolv}
\end{tabular}
\caption{Baseline English Results}
\label{table:englishtask}
\end{table}

\begin{table}
\begin{tabular}{l|l|l}%
\bfseries Method & \bfseries Mean Error & \bfseries Correlation
\csvreader[head to column names]{results/preprocessing_eval_sorted.csv}{}
{\\\hline\csvcoli&\csvcoliii&\csvcolv}
\end{tabular}
\caption{Results on Our Task}
\label{table:ourtask}
\end{table}

\begin{table}
\begin{tabular}{l|l|l}%
\bfseries Method & \bfseries Mean Error & \bfseries Correlation
\csvreader[head to column names]{results/preprocessing_eval_1_sorted.csv}{}
{\\\hline\csvcoli&\csvcoliii&\csvcolv}
\end{tabular}
\caption{Results on ws353 Task}
\label{table:ws353task}
\end{table}

