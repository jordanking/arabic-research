\section{Introduction}


Arabic word embeddings are numerical vector representations of a word's meaning - both semantic meaning and syntactic meaning. These embeddings are obtained using machine learning algorithms - word2vec - that utilize the context a word appears in to infer its meaning \cite{mikolovdist:2013, mikoloveffic:2013}. This works very well as words with similar meanings tend to be used in similar contexts, which are defined by the preceeding and following $n$ words. For example the sentences \textit{I eat bread every night} and \textit{I eat rice every night} are examples of how food words may appear in similar contexts. With enough text to process, we can train numerical vectors to learn that bread and rice appear in these \textit{common-for-food} contexts. Similarly, we can learn syntactic relationships because different parts of speech appear in certain context patterns as well.
\\
High quality word embeddings provide a representation of the meaning of a word, without ever translating or referencing a dictionary. We can obtain the semantic and syntactic meaning directly from a corpus of natural written language. With accurate word embeddings, we can perform powerful operations to investigate the relationships between words in a corpus. A few of the possible operations are measuring the similarity of two words, identifying which word from a set is least similar, and solving basic analogies. The classic demonstration of word embeddings is to take (the embeddings of) \textit{king}, subtract \textit{man}, and add \textit{woman}. The resulting embedding is closest to the embedding for \textit{queen}. Intuitively, this allows us to subtract the male gender meaning from king's embedding, add the female gender meaning, and end up with an embedding equivalent to queen's embedding.
\\
We would like accurate Arabic word embeddings so we can interpret the general topics of discussion in Arabic media without using translation or ignoring some words belonging to a topic. An example application would be to use the embeddings to learn what words are highly similar to words similar to fear (in Arabic), and then compute the degree to which some media is using fearful language in the context of political or economic termoil.
\\
Methodologies and properties of English word embeddings have been extensively researched, however little attention has been given to the production and application of Arabic word embeddings. Written Arabic words often carry more contextual information about objects, tense, gender, and definiteness than English, meaning that Arabic unigrams occur less frequently on average than English unigrams. This has a significant effect on the training and application of Arabic word embeddings, as the embeddings are trained on unigram tokens.
\\
The contributions of this work are as follows: 1) We perform a comparative empirical evaluation of Arabic and English word vectors using both a semantic similarity task and a syntactic similarity task. We show that standard parameters for English word embeddings lead to poor Arabic word embeddings. 2) We develop and open-source a simple software package that provides easy access to important Arabic natural language processing tools. 3) We present a semantic similarity task using a team of native Arabic speakers. This task is larger than other published tasks and uses multiple native speakers to manually provide high quality human labels. 4) We present an empirical analysis identifying the parameters that are most effective for our tasks, identifying a set of best practices for training Arabic word embeddings.